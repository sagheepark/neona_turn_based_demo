"""
Incremental Knowledge Cache Service
Implements greeting-triggered caching and incremental knowledge addition for improved RAG performance
"""

import re
from datetime import datetime
from typing import Dict, List, Optional
from .knowledge_service import KnowledgeService


class SessionKnowledge:
    """Data structure for session-specific knowledge cache"""
    
    def __init__(self):
        self.knowledge_base = {}  # topic -> knowledge_items
        self.topic_history = []   # chronological topic tracking
        self.last_updated = datetime.now()
        self.total_items = 0


class IncrementalKnowledgeCache:
    """
    Core caching system that:
    - Proactively caches knowledge from greetings
    - Incrementally adds knowledge as new topics emerge
    - Smart relevance scoring for cached knowledge reuse
    """
    
    def __init__(self):
        self.session_cache = {}  # session_id -> SessionKnowledge
        self.knowledge_service = KnowledgeService()
    
    def cache_greeting_knowledge(self, session_id: str, greeting: str, character_id: str) -> List[Dict]:
        """Extract topics from greeting and cache relevant knowledge"""
        
        # Extract topics from greeting
        topics = self.extract_topics(greeting)
        print(f"ğŸ­ Greeting topics extracted: {topics}")
        
        # Initialize session cache if not exists
        if session_id not in self.session_cache:
            self.session_cache[session_id] = SessionKnowledge()
        
        session_knowledge = self.session_cache[session_id]
        
        # Retrieve knowledge for all topics
        cached_items = []
        for topic in topics:
            if topic and topic not in session_knowledge.knowledge_base:
                knowledge_items = self.knowledge_service.search_relevant_knowledge(
                    topic, character_id, max_results=3
                )
                
                if knowledge_items:
                    session_knowledge.knowledge_base[topic] = knowledge_items
                    session_knowledge.topic_history.append({
                        'topic': topic,
                        'added_at': datetime.now(),
                        'message_context': greeting[:100]
                    })
                    session_knowledge.total_items += len(knowledge_items)
                    cached_items.extend(knowledge_items)
                    
                    print(f"ğŸ“š Cached {len(knowledge_items)} items for topic: {topic}")
        
        session_knowledge.last_updated = datetime.now()
        print(f"âœ… Cached {len(cached_items)} total knowledge items from greeting")
        
        return cached_items
    
    def add_knowledge_incrementally(self, session_id: str, user_message: str, character_id: str) -> List[Dict]:
        """Add new knowledge only for uncached topics"""
        
        if session_id not in self.session_cache:
            self.session_cache[session_id] = SessionKnowledge()
        
        session_knowledge = self.session_cache[session_id]
        
        # Extract new topics from current message
        new_topics = self.extract_topics(user_message)
        
        # Check which topics are not in cache
        uncached_topics = []
        for topic in new_topics:
            if topic and topic not in session_knowledge.knowledge_base:
                uncached_topics.append(topic)
        
        if uncached_topics:
            print(f"ğŸ†• Adding knowledge for new topics: {uncached_topics}")
            
            # Retrieve knowledge for new topics only
            for topic in uncached_topics:
                knowledge_items = self.knowledge_service.search_relevant_knowledge(
                    topic, character_id, max_results=3
                )
                
                if knowledge_items:
                    session_knowledge.knowledge_base[topic] = knowledge_items
                    session_knowledge.topic_history.append({
                        'topic': topic,
                        'added_at': datetime.now(),
                        'message_context': user_message[:100]
                    })
                    session_knowledge.total_items += len(knowledge_items)
                    
                    print(f"ğŸ“š Cached {len(knowledge_items)} items for topic: {topic}")
        
        session_knowledge.last_updated = datetime.now()
        return self.get_relevant_cached_knowledge(session_id, user_message)
    
    def get_relevant_cached_knowledge(self, session_id: str, user_message: str) -> List[Dict]:
        """Get relevant knowledge from accumulated cache"""
        
        if session_id not in self.session_cache:
            return []
        
        session_knowledge = self.session_cache[session_id]
        user_topics = self.extract_topics(user_message)
        
        # Collect relevant knowledge from multiple topics
        relevant_knowledge = []
        
        for topic in session_knowledge.knowledge_base:
            # Calculate topic relevance to current message
            relevance = self.calculate_topic_relevance(topic, user_topics, user_message)
            
            if relevance > 0.3:  # Relevance threshold
                knowledge_items = session_knowledge.knowledge_base[topic]
                for item in knowledge_items:
                    relevant_knowledge.append({
                        **item,
                        'cache_topic': topic,
                        'relevance_score': relevance
                    })
        
        # Sort by relevance and limit results
        relevant_knowledge.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        result = relevant_knowledge[:5]  # Top 5 most relevant
        print(f"ğŸ¯ Using {len(result)} cached knowledge items")
        return result
    
    def extract_topics(self, text: str) -> List[str]:
        """Extract meaningful topics from text"""
        if not text or not text.strip():
            return []
        
        # Define Korean topic patterns
        topic_patterns = [
            r'3Â·1\s*ìš´ë™',
            r'ìœ ê´€ìˆœ',
            r'ë…ë¦½ìš´ë™',
            r'ì„ì‹œì •ë¶€',
            r'ì¼ì œê°•ì ê¸°',
            r'ì—­ì‚¬',
            r'ì¡°ì„ ì‹œëŒ€',
            r'ê³ êµ¬ë ¤',
            r'ë°±ì œ',
            r'ì‹ ë¼',
        ]
        
        topics = []
        
        # Extract specific historical topics
        for pattern in topic_patterns:
            matches = re.findall(pattern, text)
            topics.extend(matches)
        
        # Extract general topics (nouns that might be relevant)
        general_patterns = [
            r'\w+ìš´ë™',  # movements
            r'\w+ì‹œëŒ€',  # eras
            r'\w+ì™•ì¡°',  # dynasties
            r'\w+ì „ìŸ',  # wars
            r'\w+ì‚¬ê±´',  # events
        ]
        
        for pattern in general_patterns:
            matches = re.findall(pattern, text)
            topics.extend(matches)
        
        # Extract question/temporal words that are topically relevant
        temporal_patterns = [
            r'ì–¸ì œ',
            r'ì‹œê¸°',
            r'ë‚ ì§œ',
            r'ì‹œê°„',
            r'ë…„ë„',
        ]
        
        for pattern in temporal_patterns:
            matches = re.findall(pattern, text)
            topics.extend(matches)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_topics = []
        for topic in topics:
            if topic and topic not in seen:
                seen.add(topic)
                unique_topics.append(topic)
        
        return unique_topics
    
    def calculate_topic_relevance(self, cached_topic: str, user_topics: List[str], user_message: str) -> float:
        """Calculate relevance of cached topic to current user message"""
        relevance = 0.0
        
        # Direct topic match
        if cached_topic in user_topics:
            relevance += 0.8
        
        # Substring match in user message
        if cached_topic in user_message:
            relevance += 0.6
        
        # Related topic heuristics
        related_topics = {
            "3Â·1 ìš´ë™": ["ë…ë¦½ìš´ë™", "ìœ ê´€ìˆœ", "ì¼ì œê°•ì ê¸°", "ë§Œì„¸", "ì‹œìœ„"],
            "ìœ ê´€ìˆœ": ["3Â·1 ìš´ë™", "ë…ë¦½ìš´ë™", "í•™ìƒ", "ì²œì•ˆ"],
            "ë…ë¦½ìš´ë™": ["3Â·1 ìš´ë™", "ìœ ê´€ìˆœ", "ì„ì‹œì •ë¶€", "ì¼ì œê°•ì ê¸°"],
            "ì—­ì‚¬": ["3Â·1 ìš´ë™", "ìœ ê´€ìˆœ", "ë…ë¦½ìš´ë™", "ì¡°ì„ ì‹œëŒ€", "ì‹œëŒ€", "ê³¼ê±°"]
        }
        
        if cached_topic in related_topics:
            for related in related_topics[cached_topic]:
                if related in user_message:
                    relevance += 0.3
                    break  # Only add once for related topics
        
        # Temporal keywords that might relate to historical topics
        temporal_keywords = ["ì–¸ì œ", "ì‹œê¸°", "ë‚ ì§œ", "ë…„ë„", "ì‹œê°„"]
        if any(keyword in user_message for keyword in temporal_keywords):
            if any(historical in cached_topic for historical in ["ìš´ë™", "ì‹œëŒ€", "ì „ìŸ", "ì‚¬ê±´"]):
                relevance += 0.2
        
        # Cap relevance at 1.0
        return min(relevance, 1.0)